{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc24721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Setup\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. Model Definitions\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, feature_maps):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, feature_maps*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*8), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_maps*8, feature_maps*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*4), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_maps*4, feature_maps*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_maps*2, feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(feature_maps, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, feature_maps):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, feature_maps, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_maps, feature_maps*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*2), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_maps*2, feature_maps*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*4), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_maps*4, feature_maps*8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_maps*8), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(feature_maps*8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "# 3. Training Function\n",
    "def train(data_path, out_dir, epochs=50, batch_size=128, lr=2e-4,\n",
    "          latent_dim=100, gf_maps=64, df_maps=64, sample_interval=5):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # Data loader\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64,64)), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    class CatDataset(Dataset):\n",
    "        def __init__(self, file_list, transform=None):\n",
    "            self.file_list = file_list\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.file_list)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path = self.file_list[idx]\n",
    "            img = Image.open(path)\n",
    "            return transform(img)\n",
    "    \n",
    "    files = [os.path.join(data_path, line.strip()) for line in os.listdir(data_path)]\n",
    "    dataset = CatDataset(files, transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize models\n",
    "    gen = Generator(latent_dim, 3, gf_maps).to(device)\n",
    "    disc = Discriminator(3, df_maps).to(device)\n",
    "    opt_g = optim.Adam(gen.parameters(), lr=lr, betas=(0.5,0.999))\n",
    "    opt_d = optim.Adam(disc.parameters(), lr=lr, betas=(0.5,0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        for imgs in loader:\n",
    "            bs = imgs.size(0)\n",
    "            real = imgs.to(device)\n",
    "            real_labels = torch.ones(bs, device=device)\n",
    "            fake_labels = torch.zeros(bs, device=device)\n",
    "            # Train Discriminator\n",
    "            z = torch.randn(bs, latent_dim, 1, 1, device=device)\n",
    "            fake = gen(z)\n",
    "            d_loss = criterion(disc(real), real_labels) + criterion(disc(fake.detach()), fake_labels)\n",
    "            disc.zero_grad(); d_loss.backward(); opt_d.step()\n",
    "            # Train Generator\n",
    "            g_loss = criterion(disc(fake), real_labels)\n",
    "            gen.zero_grad(); g_loss.backward(); opt_g.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | D: {d_loss:.4f} | G: {g_loss:.4f} | calculation time {round((time.time() - start_time) / 60, 2)} min\")\n",
    "        if (epoch+1) % sample_interval == 0:\n",
    "            path = os.path.join(out_dir, f\"sample_{epoch+1}.png\")\n",
    "            save_image(utils.make_grid(fake[:16], normalize=True), path)\n",
    "    # Save models\n",
    "    torch.save(gen.state_dict(), os.path.join(out_dir, \"generator.pth\"))\n",
    "    torch.save(disc.state_dict(), os.path.join(out_dir, \"discriminator.pth\"))\n",
    "    return gen, disc\n",
    "\n",
    "# 4. Quantitative Evaluation (FID)\n",
    "def evaluate_fid(real_path, fake_path, batch_size=50):\n",
    "    fid = calculate_fid_given_paths([real_path, fake_path], batch_size, device, dims=2048)\n",
    "    print(f\"FID: {fid:.3f}\")\n",
    "    return fid\n",
    "\n",
    "# 5. Mode Collapse Check\n",
    "def check_mode_collapse(gen, latent_dim, num_samples=1000, threshold=0.9):\n",
    "    gen.eval()\n",
    "    zs = torch.randn(num_samples, latent_dim, 1, 1, device=device)\n",
    "    with torch.no_grad():\n",
    "        sims = cosine_similarity(gen(zs).reshape(num_samples, -1).cpu().numpy())\n",
    "    high = np.sum(sims > threshold) - num_samples\n",
    "    all = num_samples * (num_samples - 1)\n",
    "    print(f\"High-similarity pairs: {high}/{all}\")\n",
    "    return high\n",
    "\n",
    "# 6. Latent-Space Interpolation\n",
    "def interpolate(gen, z1, z2, steps=10):\n",
    "    gen.eval()\n",
    "    alphas = torch.linspace(0,1,steps, device=device).view(-1,1,1,1)\n",
    "    zs = (1-alphas)*z1 + alphas*z2\n",
    "    with torch.no_grad(): imgs = gen(zs).cpu()\n",
    "    grid = utils.make_grid(imgs, normalize=True, nrow=steps)\n",
    "    return grid\n",
    "\n",
    "# 7. Hyperparameter Grid Search\n",
    "def grid_search(params):\n",
    "    import itertools\n",
    "    best = {'fid': float('inf'), 'params': None}\n",
    "    for lr, bs in itertools.product(params['lr'], params['batch_size']):\n",
    "        # Run training & FID eval here\n",
    "        fid = np.random.rand()*100  # placeholder\n",
    "        if fid < best['fid']:\n",
    "            best.update({'fid': fid, 'params': {'lr': lr, 'batch_size': bs}})\n",
    "    print(best)\n",
    "    return best\n",
    "\n",
    "def create_real_subset_for_fid(data_dir):\n",
    "\n",
    "    # Set seed\n",
    "    random.seed(42)\n",
    "    sample_size = 1000\n",
    "\n",
    "    temp_real_path = Path(data_dir).parent / \"real_subset_for_fid\"\n",
    "    if temp_real_path.exists():\n",
    "        shutil.rmtree(temp_real_path)\n",
    "    temp_real_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    real_images = os.listdir(data_dir)\n",
    "    sampled_images = random.sample(real_images, sample_size)\n",
    "\n",
    "    for img_path in sampled_images:\n",
    "        shutil.copy(f'{data_dir}/{img_path}', temp_real_path / img_path)\n",
    "\n",
    "DATA_DIR = './cats_data'\n",
    "OUT_DIR  = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adb4b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | D: 0.3554 | G: 1.7254 | calculation time 6.31 min\n",
      "Epoch 2/50 | D: 0.8433 | G: 1.4582 | calculation time 6.66 min\n",
      "Epoch 3/50 | D: 0.9313 | G: 2.5497 | calculation time 6.82 min\n",
      "Epoch 4/50 | D: 0.7322 | G: 6.8336 | calculation time 7.16 min\n",
      "Epoch 5/50 | D: 1.3214 | G: 9.0833 | calculation time 7.52 min\n",
      "Epoch 6/50 | D: 0.1538 | G: 5.3249 | calculation time 7.64 min\n",
      "Epoch 7/50 | D: 0.5576 | G: 6.0926 | calculation time 7.72 min\n",
      "Epoch 8/50 | D: 0.2614 | G: 3.5190 | calculation time 7.81 min\n",
      "Epoch 9/50 | D: 0.3989 | G: 2.9087 | calculation time 7.88 min\n",
      "Epoch 10/50 | D: 0.0293 | G: 4.8368 | calculation time 7.91 min\n",
      "Epoch 11/50 | D: 0.3420 | G: 3.3074 | calculation time 7.96 min\n",
      "Epoch 12/50 | D: 0.4660 | G: 8.2312 | calculation time 8.05 min\n",
      "Epoch 13/50 | D: 0.3022 | G: 7.6864 | calculation time 7.76 min\n",
      "Epoch 14/50 | D: 0.0446 | G: 6.3629 | calculation time 7.89 min\n",
      "Epoch 15/50 | D: 0.7125 | G: 7.2566 | calculation time 8.08 min\n",
      "Epoch 16/50 | D: 0.2614 | G: 5.2256 | calculation time 8.03 min\n",
      "Epoch 17/50 | D: 0.1708 | G: 4.9589 | calculation time 8.01 min\n",
      "Epoch 18/50 | D: 0.3121 | G: 6.5171 | calculation time 8.0 min\n",
      "Epoch 19/50 | D: 0.1808 | G: 7.5482 | calculation time 7.99 min\n",
      "Epoch 20/50 | D: 0.1274 | G: 5.9304 | calculation time 7.27 min\n",
      "Epoch 21/50 | D: 0.3229 | G: 12.0244 | calculation time 7.26 min\n",
      "Epoch 22/50 | D: 0.0238 | G: 8.3570 | calculation time 7.27 min\n",
      "Epoch 23/50 | D: 0.1656 | G: 8.6826 | calculation time 7.26 min\n",
      "Epoch 24/50 | D: 0.0790 | G: 4.5572 | calculation time 7.27 min\n",
      "Epoch 25/50 | D: 0.1214 | G: 4.9861 | calculation time 7.27 min\n",
      "Epoch 26/50 | D: 0.0402 | G: 5.1832 | calculation time 7.3 min\n",
      "Epoch 27/50 | D: 0.6710 | G: 5.2366 | calculation time 7.31 min\n",
      "Epoch 28/50 | D: 0.2581 | G: 11.3827 | calculation time 7.29 min\n",
      "Epoch 29/50 | D: 0.3866 | G: 3.7511 | calculation time 7.31 min\n",
      "Epoch 30/50 | D: 0.0628 | G: 4.5025 | calculation time 7.3 min\n",
      "Epoch 31/50 | D: 0.0289 | G: 6.4858 | calculation time 7.3 min\n",
      "Epoch 32/50 | D: 0.8262 | G: 0.0221 | calculation time 7.3 min\n",
      "Epoch 33/50 | D: 0.0859 | G: 4.0836 | calculation time 7.3 min\n",
      "Epoch 34/50 | D: 0.9689 | G: 0.5404 | calculation time 7.3 min\n",
      "Epoch 35/50 | D: 0.0739 | G: 6.1842 | calculation time 7.3 min\n",
      "Epoch 36/50 | D: 0.1515 | G: 4.1066 | calculation time 7.28 min\n",
      "Epoch 37/50 | D: 0.7201 | G: 2.2897 | calculation time 7.28 min\n",
      "Epoch 38/50 | D: 0.0908 | G: 4.8939 | calculation time 7.26 min\n",
      "Epoch 39/50 | D: 0.1649 | G: 6.7847 | calculation time 7.26 min\n",
      "Epoch 40/50 | D: 0.2461 | G: 3.7128 | calculation time 7.1 min\n",
      "Epoch 41/50 | D: 0.1874 | G: 7.1254 | calculation time 6.98 min\n",
      "Epoch 42/50 | D: 0.5161 | G: 0.0384 | calculation time 7.06 min\n",
      "Epoch 43/50 | D: 0.5714 | G: 0.9851 | calculation time 7.3 min\n",
      "Epoch 44/50 | D: 0.1364 | G: 5.1552 | calculation time 7.57 min\n",
      "Epoch 45/50 | D: 0.0172 | G: 6.6626 | calculation time 7.88 min\n",
      "Epoch 46/50 | D: 0.0862 | G: 8.8995 | calculation time 7.96 min\n",
      "Epoch 47/50 | D: 0.1399 | G: 5.3797 | calculation time 7.86 min\n",
      "Epoch 48/50 | D: 0.1071 | G: 9.2634 | calculation time 7.93 min\n",
      "Epoch 49/50 | D: 0.9051 | G: 0.2932 | calculation time 8.03 min\n",
      "Epoch 50/50 | D: 0.1063 | G: 8.1885 | calculation time 8.08 min\n"
     ]
    }
   ],
   "source": [
    "# Full Pipeline\n",
    "# 1) Train on cats\n",
    "\n",
    "gen, disc = train(DATA_DIR, OUT_DIR, epochs=50, batch_size=128, lr=2e-4) # 10 epochs -> 74 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fc88956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Generate and save a fresh batch of samples\n",
    "\n",
    "fixed_z = torch.randn(64, 100, 1, 1, device=device)\n",
    "fresh_samples = gen(fixed_z)\n",
    "save_image(utils.make_grid(fresh_samples, normalize=True), os.path.join(OUT_DIR, 'fresh_samples.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8c59c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 132.665\n"
     ]
    }
   ],
   "source": [
    "# 3) Quantitative evaluation (FID)\n",
    "\n",
    "fake_dir = os.path.join(OUT_DIR, 'generated_for_fid')\n",
    "os.makedirs(fake_dir, exist_ok=True)\n",
    "#    Generate a set of images for FID\n",
    "with torch.no_grad():\n",
    "    z_fid = torch.randn(1000, 100, 1, 1, device=device)\n",
    "    imgs_fid = gen(z_fid).cpu()\n",
    "    for i, img in enumerate(imgs_fid):\n",
    "        save_image(img, os.path.join(fake_dir, f\"fid_{i}.png\"))\n",
    "fid_score = evaluate_fid(real_path='real_subset_for_fid', fake_path=fake_dir, batch_size=50)\n",
    "\n",
    "# 30 epochs -> 114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e1eb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-similarity pairs: 2412/999000\n"
     ]
    }
   ],
   "source": [
    "# 4) Mode-collapse check\n",
    "\n",
    "too_many_similar = check_mode_collapse(gen, latent_dim=100, num_samples=1000, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f18337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Latent-space interpolation (save grid)\n",
    "\n",
    "z1 = torch.randn(1, 100, 1, 1, device=device)\n",
    "z2 = torch.randn(1, 100, 1, 1, device=device)\n",
    "interp_grid = interpolate(gen, z1, z2, steps=10)\n",
    "save_image(interp_grid, os.path.join(OUT_DIR, 'interpolation.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO # 6) Hyperparameter grid search \n",
    "\n",
    "tune_params = {'lr': [1e-4, 2e-4, 5e-4], 'batch_size': [64, 128]}\n",
    "best = grid_search(tune_params)\n",
    "print(f\"Best hyperparameters: {best['params']} with FID {best['fid']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
